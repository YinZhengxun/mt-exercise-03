2025-05-05 15:21:53,121 - INFO - root - Hello! This is Joey-NMT (version 2.2.0).
2025-05-05 15:21:53,121 - INFO - joeynmt.data - Building tokenizer...
2025-05-05 15:21:53,132 - INFO - joeynmt.tokenizers - de tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-05 15:21:53,132 - INFO - joeynmt.tokenizers - en tokenizer: SubwordNMTTokenizer(level=bpe, lowercase=False, normalize=False, filter_by_length=(-1, 100), pretokenizer=none, tokenizer=BPE, separator=@@, dropout=0.0)
2025-05-05 15:21:53,133 - INFO - joeynmt.data - Building vocabulary...
2025-05-05 15:21:53,350 - INFO - joeynmt.data - Loading dev set...
2025-05-05 15:21:53,359 - INFO - joeynmt.data - Loading test set...
2025-05-05 15:21:53,377 - INFO - joeynmt.data - Data loaded.
2025-05-05 15:21:53,377 - INFO - joeynmt.data - Train dataset: None
2025-05-05 15:21:53,377 - INFO - joeynmt.data - Valid dataset: PlaintextDataset(split=dev, len=500, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-05 15:21:53,379 - INFO - joeynmt.data -  Test dataset: PlaintextDataset(split=test, len=2999, src_lang=de, trg_lang=en, has_trg=True, random_subset=-1)
2025-05-05 15:21:53,379 - INFO - joeynmt.data - First 10 Src tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) die (9) der
2025-05-05 15:21:53,379 - INFO - joeynmt.data - First 10 Trg tokens: (0) <unk> (1) <pad> (2) <s> (3) </s> (4) , (5) . (6) the (7) in (8) die (9) der
2025-05-05 15:21:53,379 - INFO - joeynmt.data - Number of unique Src tokens (vocab_size): 4117
2025-05-05 15:21:53,379 - INFO - joeynmt.data - Number of unique Trg tokens (vocab_size): 4117
2025-05-05 15:21:53,381 - INFO - joeynmt.model - Building an encoder-decoder model...
2025-05-05 15:21:53,458 - INFO - joeynmt.model - Enc-dec model built.
2025-05-05 15:21:53,555 - INFO - joeynmt.helpers - Load model from C:\Users\lenovo\mt-exercise\mt-exercise-03\models\deen_transformer_prenorm\40500.ckpt.
2025-05-05 15:21:53,566 - INFO - joeynmt.prediction - Model(
	encoder=TransformerEncoder(num_layers=4, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	decoder=TransformerDecoder(num_layers=1, num_heads=2, alpha=1.0, layer_norm="pre", activation=ReLU()),
	src_embed=Embeddings(embedding_dim=256, vocab_size=4117),
	trg_embed=Embeddings(embedding_dim=256, vocab_size=4117),
	loss_function=None)
2025-05-05 15:21:53,569 - INFO - joeynmt.prediction - Decoding on dev set...
2025-05-05 15:21:53,569 - INFO - joeynmt.prediction - Predicting 500 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-05 15:28:26,756 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 393.1758[sec], evaluation: 0.0000[sec]
2025-05-05 15:28:26,756 - INFO - joeynmt.prediction - Decoding on test set...
2025-05-05 15:28:26,756 - INFO - joeynmt.prediction - Predicting 2999 example(s)... (Beam search with beam_size=5, beam_alpha=1.0, n_best=1, min_output_length=1, max_output_length=-1, return_prob='none', generate_unk=True, repetition_penalty=-1, no_repeat_ngram_size=-1)
2025-05-05 16:13:34,925 - INFO - joeynmt.prediction - Evaluation result (beam search) , generation: 2708.1113[sec], evaluation: 0.0000[sec]
